{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 3 SALES FORECASTS AND LATENT FEATURE IDENTITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PURPOSE:  \n",
    "This program implements a embedded neural network in tensorflow to perform a store by store\n",
    "sales forecast.  This is not an inferential program although it measures accuracy against a test set in mean \n",
    "average percentage error.  The program tests for potential sales accuracy if data additional to the base \n",
    "accounting data could be developed.  The latent features and store scaling measures the impace of unknown\n",
    "features available to the store management but not available from the accounting data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### INPUT: \n",
    "Features developed in the prior programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OUTPUT: \n",
    "Sales forecasts and accutacy on a test set of known data and identification of latent (or unknown) variables on the sales forecasts by impact strength, store id, and date of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import standard python and sklearn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tensorflow and tensorflow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,callbacks,losses,optimizers,initializers,models,regularizers\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,Embedding,Flatten,concatenate,Input\n",
    "from tensorflow.keras.callbacks import CSVLogger,ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.optimizers import SGD,RMSprop,Adam,Adamax\n",
    "#from tensorflow.train import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal,RandomUniform,TruncatedNormal\n",
    "from tensorflow.keras.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed for initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for gpu and expect this output:\n",
    "\n",
    "[\n",
    "  name: \"/cpu:0\"device_type: \"CPU\",\n",
    "  name: \"/gpu:0\"device_type: \"GPU\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4964303642562140285\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6087873103136677902\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_feather(os.getcwd() + '/agg_data/' + 'Features.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify categorical, continuous, data and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vars(dt):\n",
    "    cat_vars = ['STORENUMBER', 'WEEKDAY', 'HOLIDAY', 'YEAR', 'WEEKOFYR', 'MONOFYR', 'DAY', 'BEFOREHOLIDAY',\n",
    "       'AFTERHOLIDAY', 'DAYSAFTEROPEN', 'DAYSBEFORECLOSE', 'LTS','LTAS']\n",
    "    cont_vars = ['DAYSINSAMPLE']\n",
    "    dep = ['SALES']\n",
    "    date = ['DATE']\n",
    "    dt = dt[cat_vars + cont_vars + dep + date].copy()\n",
    "    dt.sort_values(by=['STORENUMBER','DATE'],inplace=True)\n",
    "    dt.reset_index(drop=True,inplace=True)\n",
    "    return dt,cat_vars,cont_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,cat_vars,cont_vars = label_vars(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create categorical embed maximum length,embedding dict, and categorical map function of labelencoder to set number of categories in each category feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_data(df,cat_vars):\n",
    "    cat_emb_max = [len(df[c].unique()) for c in cat_vars]\n",
    "    cat_vars_dict = dict(zip(cat_vars,cat_emb_max))\n",
    "    cat_map = [(c,LabelEncoder()) for c in cat_vars]\n",
    "    return cat_vars_dict,cat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars_dict,cat_map = cat_data(df,cat_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create continuous data map function with minmaxwscaler and range default to 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_data(cont_vars,mn=0,mx=1):\n",
    "    cont_map = [([c],MinMaxScaler(feature_range = (mn,mx),copy=False)) for c in cont_vars]\n",
    "    return cont_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_map = cont_data(cont_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit map function to continuous and categorical features but do not apply transform until after data is split into train, validate and test.  This fits labels and scaled range to entire data set rather than train,validate and test separtely.  DataFrameMapper from sklearn-pandas will only transform the features by column label inluded in the category and continuous feature lists called cat_map or cont_map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vars_mapped(cat_map,cont_map,dt):\n",
    "    cat_mapper = DataFrameMapper(cat_map)\n",
    "    cat_map_fit = cat_mapper.fit(dt)\n",
    "    cont_mapper = DataFrameMapper(cont_map)\n",
    "    cont_map_fit = cont_mapper.fit(dt)\n",
    "    return cat_map_fit,cont_map_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ignore Data conversion warning \n",
    "It is expected since continuous feature is a int64 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "cat_map_fit,cont_map_fit = vars_mapped(cat_map,cont_map,dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set train, test, validate dates with validation as last 56 days and test at last 14 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dates(dt,vstart=72,tstart=14):\n",
    "    dates = list(dt.DATE.unique())\n",
    "    dates.sort()\n",
    "    dates_validate = dates[-vstart:-tstart]\n",
    "    dates_test = dates[-tstart:]\n",
    "    dates_train = dates[:-vstart]\n",
    "    return dates_train,dates_validate,dates_test,dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train,dates_validate,dates_test,dates = split_dates(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset by train,validate,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dt,dates_train,dates_validate,dates_test):      \n",
    "    data = dt.sort_values(by=['STORENUMBER','DATE'])\n",
    "    data_train = data.loc[data.DATE.isin(dates_train)]\n",
    "    data_validate = data.loc[data.DATE.isin(dates_validate)]\n",
    "    data_test = data.loc[data.DATE.isin(dates_test)]\n",
    "    return data_train,data_validate,data_test,data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train,data_validate,data_test,data = split_data(dt,dates_train,dates_validate,dates_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode and scale data and reshape into array of vectors. \n",
    "___________________________________________________________________________________________________________\n",
    "Since the input layer of the neural network is a horizontally concatenated layer of each categorical variable in its own embedding input shared with the continuous variables each in its own dense input the train, validate and test data needs to be reshaped into a list of vectors for each feature.  To keep the array in mixed dtypes (i.e., int and float), input data is a list of arrays with each element in the list being a vector for the shared input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_shape_data(data_train,data_validate,data_test,cat_map_fit,cont_map_fit):\n",
    "    #set target variables\n",
    "    y_tr = data_train.SALES.values.reshape(-1,1)\n",
    "    y_val = data_validate.SALES.values.reshape(-1,1)\n",
    "    y_ts = data_test.SALES.values.reshape(-1,1)\n",
    "    #transform categorical data\n",
    "    cat_train = cat_map_fit.transform(data_train).astype(np.int64)\n",
    "    cat_validate = cat_map_fit.transform(data_validate).astype(np.int64)\n",
    "    cat_test = cat_map_fit.transform(data_test).astype(np.int64)\n",
    "    #transform continuous variables\n",
    "    cont_train = cont_map_fit.transform(data_train).astype(np.float32)\n",
    "    cont_validate = cont_map_fit.transform(data_validate).astype(np.float32)\n",
    "    cont_test = cont_map_fit.transform(data_test).astype(np.float32)\n",
    "    #combine categorical and continuous data into array of vectors\n",
    "    data_tr = np.hsplit(cat_train,cat_train.shape[1])+np.hsplit(cont_train,cont_train.shape[1])\n",
    "    data_val = np.hsplit(cat_validate,cat_validate.shape[1])+np.hsplit(cont_validate,cont_validate.shape[1])\n",
    "    data_ts = np.hsplit(cat_test,cat_test.shape[1])+np.hsplit(cont_test,cont_test.shape[1])\n",
    "    return y_tr,y_val,y_ts,data_tr,data_val,data_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr,y_val,y_ts,data_tr,data_val,data_ts = map_shape_data(data_train,data_validate,data_test,cat_map_fit,cont_map_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create single input vector (input_shape = 1) for categorical input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_input(feat,cat_vars_dict):\n",
    "    name = feat[0]\n",
    "    c1 = len(feat[1].classes_)\n",
    "    c2 = cat_vars_dict[name]\n",
    "    if c2 > 50:c2 = 50\n",
    "    inp = Input(shape=(1,),dtype='int64',name=name + '_in')\n",
    "    #no third dimension for a time distributed series so flattened into column of 1\n",
    "    #embedding layer is map of number of classes (c) to number of embedded features (c2)\n",
    "    u = Flatten(name=name+'_flt')(Embedding(c1,c2,input_length=1)(inp))\n",
    "    return inp,u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of Input,Flatten,and Embedding layers for the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dennis/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "embs = [cat_input(feat,cat_vars_dict) for feat in cat_map_fit.features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deprecation warning is an incompatibility between keras and tensorflow.keras.  The error message is an outstanding bug in tensorflow and does not occur in keras.  Tensorflow has an open issue report regarding this error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create Input and Dense layer for continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_input(feat):\n",
    "    name = feat[0][0]\n",
    "    inp = Input((1,), name=name+'_in')\n",
    "    d = Dense(1, name = name + '_d')(inp)\n",
    "    return inp,d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of Input and Dense layers for continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = [cont_input(feat) for feat in cont_map_fit.features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a four layer model using a shared input layer for the categorical and continuous variables.  The hideen 2 layers are high node counts because sample count in input data is large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_model(conts,embs):\n",
    "    #concatenate the inputs and embedded layers with the inputs and continuous dense layers\n",
    "    #referred to as 'shared layers' in tensorflow.keras documentation\n",
    "    x = concatenate([emb for inp,emb in embs] + [d for inp,d in conts])\n",
    "    #apply L2 normalization using the BatchNormalization method on continuous features\n",
    "    x = Dense(1000, activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)\n",
    "    #apply small dropout for first normalization\n",
    "    x = Dropout(rate=0.1)(x)\n",
    "    #apply additional L2 normalization using the BatchNormalization method\n",
    "    x =\tBatchNormalization()(x)\n",
    "    x = Dense(500, activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)\n",
    "    #apply small dropout for normalization\n",
    "    x =\tDropout(rate=0.1)(x)\n",
    "    #apply L2 normalization using the BatchNormalization method\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(250,activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)  \n",
    "    x =\tDropout(rate=0.1)(x)\n",
    "    #apply L2 normalization using the BatchNormalization method\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(1, activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)\n",
    "    model = Model([inp for inp,emb in embs] + [inp for inp,d in conts], x)\n",
    "    model.compile(optimizer='Adam',loss='mean_absolute_error',metrics=['mape'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement logger,reduce the learning rate when loss function change gets small,add early stopping and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dennis/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "csv_logger = CSVLogger('SF_data/SF_Error.csv')\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=3,min_lr=0.0001)\n",
    "mc = ModelCheckpoint('SF_data/SFBestModel',save_best_only=True)\n",
    "model = embed_model(conts,embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next process is cpu/gpu intensive.  This code should be run on a gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28229 samples, validate on 2553 samples\n",
      "WARNING:tensorflow:From /home/dennis/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "28229/28229 [==============================] - 12s 430us/sample - loss: 0.1206 - mean_absolute_percentage_error: 61250.2227 - val_loss: 0.0843 - val_mean_absolute_percentage_error: 21.9856\n",
      "Epoch 2/20\n",
      "28229/28229 [==============================] - 10s 352us/sample - loss: 0.0572 - mean_absolute_percentage_error: 19388.5645 - val_loss: 0.0445 - val_mean_absolute_percentage_error: 12.9249\n",
      "Epoch 3/20\n",
      "28229/28229 [==============================] - 10s 356us/sample - loss: 0.0489 - mean_absolute_percentage_error: 32884.3555 - val_loss: 0.0334 - val_mean_absolute_percentage_error: 9.0525\n",
      "Epoch 4/20\n",
      "28229/28229 [==============================] - 10s 351us/sample - loss: 0.0459 - mean_absolute_percentage_error: 41031.5742 - val_loss: 0.0449 - val_mean_absolute_percentage_error: 12.8848\n",
      "Epoch 5/20\n",
      "28229/28229 [==============================] - 12s 424us/sample - loss: 0.0450 - mean_absolute_percentage_error: 42316.5273 - val_loss: 0.0373 - val_mean_absolute_percentage_error: 10.5713\n",
      "Epoch 6/20\n",
      "28229/28229 [==============================] - 10s 363us/sample - loss: 0.0453 - mean_absolute_percentage_error: 21357.9668 - val_loss: 0.0351 - val_mean_absolute_percentage_error: 9.2953\n",
      "Epoch 7/20\n",
      "28229/28229 [==============================] - 10s 363us/sample - loss: 0.0373 - mean_absolute_percentage_error: 12105.0547 - val_loss: 0.0313 - val_mean_absolute_percentage_error: 8.4787\n",
      "Epoch 8/20\n",
      "28229/28229 [==============================] - 14s 506us/sample - loss: 0.0366 - mean_absolute_percentage_error: 26197.7969 - val_loss: 0.0305 - val_mean_absolute_percentage_error: 8.1628\n",
      "Epoch 9/20\n",
      "28229/28229 [==============================] - 13s 446us/sample - loss: 0.0356 - mean_absolute_percentage_error: 14227.9434 - val_loss: 0.0294 - val_mean_absolute_percentage_error: 8.3362\n",
      "Epoch 10/20\n",
      "28229/28229 [==============================] - 10s 363us/sample - loss: 0.0355 - mean_absolute_percentage_error: 52460.3828 - val_loss: 0.0297 - val_mean_absolute_percentage_error: 8.2696\n",
      "Epoch 11/20\n",
      "28229/28229 [==============================] - 15s 515us/sample - loss: 0.0355 - mean_absolute_percentage_error: 6247.6660 - val_loss: 0.0297 - val_mean_absolute_percentage_error: 7.9458\n",
      "Epoch 12/20\n",
      "28229/28229 [==============================] - 15s 523us/sample - loss: 0.0346 - mean_absolute_percentage_error: 37884.2695 - val_loss: 0.0292 - val_mean_absolute_percentage_error: 8.0503\n",
      "Epoch 13/20\n",
      "28229/28229 [==============================] - 14s 508us/sample - loss: 0.0342 - mean_absolute_percentage_error: 45883.8477 - val_loss: 0.0288 - val_mean_absolute_percentage_error: 7.8473\n",
      "Epoch 14/20\n",
      "28229/28229 [==============================] - 12s 441us/sample - loss: 0.0347 - mean_absolute_percentage_error: 19069.2051 - val_loss: 0.0294 - val_mean_absolute_percentage_error: 7.9512\n",
      "Epoch 15/20\n",
      "28229/28229 [==============================] - 14s 481us/sample - loss: 0.0341 - mean_absolute_percentage_error: 33669.8516 - val_loss: 0.0290 - val_mean_absolute_percentage_error: 7.8248\n",
      "Epoch 16/20\n",
      "28229/28229 [==============================] - 12s 408us/sample - loss: 0.0341 - mean_absolute_percentage_error: 36521.2578 - val_loss: 0.0290 - val_mean_absolute_percentage_error: 7.7546\n",
      "Epoch 17/20\n",
      "28229/28229 [==============================] - 15s 531us/sample - loss: 0.0338 - mean_absolute_percentage_error: 20929.3203 - val_loss: 0.0294 - val_mean_absolute_percentage_error: 7.8571\n",
      "Epoch 18/20\n",
      "28229/28229 [==============================] - 15s 527us/sample - loss: 0.0336 - mean_absolute_percentage_error: 42846.0703 - val_loss: 0.0291 - val_mean_absolute_percentage_error: 7.8038\n",
      "Epoch 19/20\n",
      "28229/28229 [==============================] - 15s 532us/sample - loss: 0.0332 - mean_absolute_percentage_error: 28650.2344 - val_loss: 0.0284 - val_mean_absolute_percentage_error: 7.8046\n",
      "Epoch 20/20\n",
      "28229/28229 [==============================] - 15s 530us/sample - loss: 0.0330 - mean_absolute_percentage_error: 38496.2578 - val_loss: 0.0293 - val_mean_absolute_percentage_error: 7.9966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa8a4503c18>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_tr,y_tr,batch_size=64,epochs=20,verbose=1,validation_data = (data_val,y_val),callbacks=[csv_logger,rlr,mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights from the modelcheckpoint save of best weights - 'save_best_only=True' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('SF_data/SFBestModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Function to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model_data,model=model):\n",
    "    pred = model.predict(model_data)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform predictions on each data set for graphing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = prediction(data_ts)\n",
    "pred_val = prediction(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to convert list of arrays to list of scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_list(arr):\n",
    "    listed = [item for sublist in arr for item in sublist]\n",
    "    return listed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to form scaled predicted and actual results by date for stores and to form latent variable dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_dataframe(data_validate,data_test,pred_test,pred_val,data):\n",
    "    pred_test = array_to_list(pred_test)\n",
    "    pred_val = array_to_list(pred_val)\n",
    "    preds = pred_val + pred_test\n",
    "    dr = pd.concat([data_validate,data_test],axis=0)\n",
    "    dr['DATE'] = data.DATE\n",
    "    dr['STORENUMBER'] = data.STORENUMBER\n",
    "    dr['SCALED_ACTUAL_SALES'] = dr.SALES\n",
    "    dr['SCALED_PREDICTED_SALES'] = preds\n",
    "    dr['LTS'] = data.LTS\n",
    "    dt['LTAS'] = data.LTAS\n",
    "    dl = dr.loc[:,['DATE','STORENUMBER','SCALED_ACTUAL_SALES','SCALED_PREDICTED_SALES','LTS','LTAS']]\n",
    "    dr = dr.loc[:,['DATE','STORENUMBER','SCALED_ACTUAL_SALES','SCALED_PREDICTED_SALES']]\n",
    "    dr.to_csv('agg_data/Scaled_Results.csv',index=False)\n",
    "    dl.to_csv('agg_data/Scaled_Latents.csv',index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store dataframe of predictions using scaled sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_dataframe(data_validate,data_test,pred_test,pred_val,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End of code: Close this file using File 'Close and Halt' from dropdown menu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
